<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Support Vector Machine (SVM)</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background: url('https://images.unsplash.com/photo-1581090464777-f3220bbe1b8b') no-repeat center center fixed;
            background-size: cover;
            color: white;
        }

        .container {
            max-width: 900px;
            background: rgba(0, 0, 0, 0.8);
            padding: 30px;
            border-radius: 12px;
            box-shadow: 0 0 15px rgba(0, 255, 255, 0.5);
            margin: 50px auto;
        }

        h1, h2 {
            color: cyan;
        }

        h2 {
            border-bottom: 2px solid cyan;
            padding-bottom: 5px;
        }

        p, li {
            font-size: 18px;
            line-height: 1.6;
        }

        .formula {
            font-weight: bold;
            color: cyan;
        }

        ul {
            margin-left: 20px;
        }
    </style>
    <!-- Load MathJax for formulas -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>
<body>
    <div class="container">
        <h1>Support Vector Machine (SVM)</h1>
        <p>
            SVM is a supervised machine learning algorithm used for both classification and regression tasks. 
            It finds the optimal <b>hyperplane</b> that separates classes by maximizing the margin between them.
        </p>

        <h2>Intuition</h2>
        <p>SVM defines three hyperplanes:</p>
        <ul>
            <li>Positive hyperplane: \[ W \cdot X + b = 1 \]</li>
            <li>Negative hyperplane: \[ W \cdot X + b = -1 \]</li>
            <li>Main hyperplane: \[ W \cdot X + b = 0 \]</li>
        </ul>
        <p>
            The margin between positive and negative hyperplanes is: 
            <span class="formula">\( d = \frac{2}{\lVert W \rVert} \)</span>
        </p>

        <h2>Types of SVM</h2>
        <ul>
            <li><b>Hard Margin SVM:</b> Requires perfectly separable data, strictly maximizes margin.</li>
            <li><b>Soft Margin SVM:</b> Allows misclassifications using slack variables \( \xi \). Trade-off controlled by hyperparameter \( C \).</li>
        </ul>

        <h2>Mathematical Intuition</h2>
        <p>The optimization problem is:</p>
        <p class="formula">\( \min \frac{1}{2} \lVert W \rVert^2 + C \sum \xi_i \)</p>
        <ul>
            <li><b>C:</b> Trade-off between margin maximization and classification error.</li>
            <li><b>\( \xi \):</b> Slack variable for misclassification.</li>
        </ul>
        <p>Hinge loss function:</p>
        <p class="formula">\( \sum_i \max(0, \; 1 - y_i (W \cdot X_i + b)) \)</p>

        <h2>Kernel Trick</h2>
        <p>
            The kernel trick maps data into higher dimensions to handle non-linearly separable data.
        </p>
        <ul>
            <li><b>Linear Kernel:</b> \( K(x, y) = x \cdot y \)</li>
            <li><b>Polynomial Kernel:</b> \( K(x, y) = (x \cdot y + c)^d \)</li>
            <li><b>RBF Kernel:</b> \( K(x, y) = e^{-\gamma \lVert x - y \rVert^2} \)</li>
        </ul>
    </div>
</body>
</html>
