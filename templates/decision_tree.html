<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Decision Tree</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 40px 20px;
      background: url('https://images.unsplash.com/photo-1529101091764-c3526daf38fe?auto=format&fit=crop&w=1950&q=80') no-repeat center center fixed;
      background-size: cover;
      color: white;
    }

    .container {
      max-width: 900px;
      margin: auto;
      background-color: rgba(0, 0, 0, 0.75);
      padding: 30px;
      border-radius: 12px;
      box-shadow: 0 0 20px rgba(0, 255, 255, 0.2);
    }

    h1, h2 {
      color: #00ffff;
    }

    h3 {
      color: #00e6e6;
    }

    p, li {
      line-height: 1.7;
    }

    ul {
      margin-bottom: 20px;
    }

    strong {
      color: #e0e0e0;
    }

    input[type="submit"] {
      display: block;
      margin: 30px auto 0 auto;
      padding: 12px 25px;
      font-size: 16px;
      background-color: #00bcd4;
      color: white;
      border: none;
      border-radius: 6px;
      cursor: pointer;
      transition: background-color 0.3s ease;
    }

    input[type="submit"]:hover {
      background-color: #0097a7;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>Decision Tree</h1>
    <p>A decision tree is a hierarchical structure used for decision-making that consists of nested if-else conditions.</p>
    <p>Mathematically, decision trees use hyperplanes that run parallel to one of the axes to split the dataset into regions.</p>

    <h2>Components of a Decision Tree:</h2>
    <ul>
      <li><strong>Root Node:</strong> The starting point representing the entire dataset.</li>
      <li><strong>Internal Nodes:</strong> Decision points based on feature values.</li>
      <li><strong>Branches:</strong> Possible outcomes leading to child nodes.</li>
      <li><strong>Leaf Nodes:</strong> Final decision or prediction values.</li>
    </ul>

    <h2>Mathematical Concepts:</h2>
    <h3>1. Entropy (Measures randomness in the data)</h3>
    <p><strong>Formula:</strong> \(E(S) = -\sum (p_i \log_2 p_i)\)</p>

    <h3>2. Gini Index (Measures impurity in a dataset)</h3>
    <p><strong>Formula:</strong> \(Gini = 1 - (P_y^2 + P_n^2)\)</p>

    <h3>3. Information Gain (IG) - Measures the quality of a split</h3>
    <p><strong>Formula:</strong> \(IG(S, A) = H(S) - \sum (|S_v| / |S|) \times H(S_v)\)</p>

    <h2>Types of Decision Trees:</h2>
    <ul>
      <li><strong>Classification Trees:</strong> Used for categorical target variables.</li>
      <li><strong>Regression Trees:</strong> Used for continuous target variables.</li>
    </ul>

    <h2>Pruning Techniques (Prevent Overfitting)</h2>
    <ul>
      <li><strong>Pre-Pruning:</strong> Stops tree growth early if further splitting does not improve results.</li>
      <li><strong>Post-Pruning:</strong> Removes unnecessary nodes after the tree is fully built.</li>
    </ul>

    <h2>Decision Tree Algorithms:</h2>
    <ul>
      <li><strong>ID3:</strong> Uses Entropy & Information Gain.</li>
      <li><strong>C4.5:</strong> An improvement over ID3 that supports continuous data.</li>
      <li><strong>CART:</strong> Uses Gini Index and supports both classification and regression.</li>
    </ul>

    <h2>Advantages:</h2>
    <ul>
      <li>Easy to interpret and visualize.</li>
      <li>Handles both numerical and categorical data.</li>
      <li>No need for feature scaling.</li>
    </ul>

    <h2>Disadvantages:</h2>
    <ul>
      <li>Prone to overfitting.</li>
      <li>Can be unstable (small changes in data can alter structure).</li>
      <li>Biased towards dominant features.</li>
    </ul>

    <form action="/decision" method="GET">
      <input type="submit" value="Check Model">
    </form>
  </div>
</body>
</html>
